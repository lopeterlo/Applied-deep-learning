{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(2, 2, 3)\n",
    ">>> mat2 = torch.randn(2, 3, 5)\n",
    ">>> res = torch.bmm(input, mat2)\n",
    ">>> res.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2545, -0.6086,  0.4410],\n",
       "         [-0.3021,  1.9885,  0.7880]],\n",
       "\n",
       "        [[ 0.4160,  0.8250,  0.5115],\n",
       "         [-2.1334, -0.1218,  1.8696]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0137,  0.9531, -1.0882, -0.4548, -1.2365],\n",
       "         [-1.0269,  0.5016,  0.1748,  1.3474,  0.7020],\n",
       "         [-0.6763,  0.9300, -0.5636, -0.7996,  1.1944]],\n",
       "\n",
       "        [[-0.0179, -0.5856,  1.5974,  1.5102, -2.0236],\n",
       "         [ 0.0426,  0.1240, -0.1549,  0.9282, -0.8846],\n",
       "         [ 1.2210,  0.6555, -0.3781, -0.2706,  0.1975]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5848, -0.1377, -0.0780, -1.0569,  0.4142],\n",
       "         [-2.2687,  1.4423,  0.2322,  2.1866,  2.7105]],\n",
       "\n",
       "        [[ 0.6522,  0.1940,  0.3434,  1.2556, -1.4706],\n",
       "         [ 2.3156,  2.4597, -4.0959, -3.8407,  4.7940]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Func\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('./data/train.jsonl', lines= True)\n",
    "valid = pd.read_json('./data/valid.jsonl', lines= True)\n",
    "test = pd.read_json('./data/test.jsonl', lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>summary</th>\n",
       "      <th>text</th>\n",
       "      <th>sent_bounds</th>\n",
       "      <th>extractive_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000000</td>\n",
       "      <td>A seven-hundred-year old oak gate at Salisbury...</td>\n",
       "      <td>The Grade I listed Harnham Gate was hit by a w...</td>\n",
       "      <td>[[0, 107], [107, 255], [255, 362]]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                            summary  \\\n",
       "0  1000000  A seven-hundred-year old oak gate at Salisbury...   \n",
       "\n",
       "                                                text  \\\n",
       "0  The Grade I listed Harnham Gate was hit by a w...   \n",
       "\n",
       "                          sent_bounds  extractive_summary  \n",
       "0  [[0, 107], [107, 255], [255, 362]]                   1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "with open(\"./glove.6B/glove.6B.300d.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add SOS and EOS\n",
    "embeddings_dict['_sos_'] =  np.random.rand(300, )\n",
    "embeddings_dict['_eos_'] =  np.random.rand(300, )\n",
    "embeddings_dict['_unk_'] =  np.random.rand(300, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class words_dict():\n",
    "    def __init__(self, glove):\n",
    "        self.word_count = {}\n",
    "        self.id_to_word = {0: '_sos_', 1: '_eos_', 2: '_unk_'}\n",
    "        self.word_to_id = {'_sos_': 0, '_eos_': 1, '_unk_': 2}\n",
    "        self.n_words = 3\n",
    "        self.tokenizer =  RegexpTokenizer(r'\\w+')\n",
    "        self.remain_id = []\n",
    "        self.glove = glove\n",
    "        \n",
    "    def add_word(self, sentence):\n",
    "        tokens = self.tokenizer.tokenize(sentence)\n",
    "        for token in tokens:\n",
    "            token = token.lower()\n",
    "            if token in self.glove.keys():\n",
    "                if not self.word_to_id.get(token) :\n",
    "                    self.word_to_id[token] = self.n_words\n",
    "                    self.id_to_word[self.n_words] = token\n",
    "                    self.n_words += 1\n",
    "                    self.word_count[token] = 1\n",
    "                else:\n",
    "                    self.word_count[token] += 1\n",
    "                    \n",
    "    def sort_dict(self):\n",
    "        self.remain_id.append(0)\n",
    "        self.remain_id.append(1)\n",
    "        self.remain_id.append(2)\n",
    "        sort_d = sorted(self.word_count.items(), key = lambda x: x[1], reverse = True)[:int(self.n_words *0.3)]\n",
    "        for (i, j) in sort_d:\n",
    "            self.remain_id.append(self.word_to_id[i])\n",
    "\n",
    "        self.reconstruct()\n",
    "    \n",
    "    \n",
    "    def reconstruct(self):\n",
    "                # reconstruct dict\n",
    "        n_words =  3\n",
    "        id_to_word = {0: '_sos_', 1: '_eos_', 2: '_unk_'}\n",
    "        word_to_id = {'_sos_': 0, '_eos_': 1, '_unk_': 2}\n",
    "        for i in self.remain_id:\n",
    "            if not word_to_id.get(i):\n",
    "                word_to_id[self.id_to_word[i]] = n_words\n",
    "                id_to_word[n_words] = self.id_to_word[i]\n",
    "                n_words += 1\n",
    "        self.n_words = n_words\n",
    "        self.id_to_word = id_to_word\n",
    "        self.word_to_id = word_to_id\n",
    "        self.remain_id = [i for i in range(n_words)]\n",
    "        \n",
    "    def get_emb(self, data):\n",
    "        if self.word_to_id.get(data, -1) != -1:\n",
    "            if self.word_to_id[data] in self.remain_id:\n",
    "                return self.glove[data]\n",
    "        return self.glove['_unk_']\n",
    "        \n",
    "    def get_word_id(self, data):\n",
    "        if data == []:\n",
    "            return -1\n",
    "        if self.word_to_id.get(data, -1) != -1:\n",
    "            if self.word_to_id[data] in self.remain_id:\n",
    "                return self.word_to_id[data]\n",
    "        return 2\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91604, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_df = train.append(valid, ignore_index= True)\n",
    "merge_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = words_dict(embeddings_dict)\n",
    "for i in range(len(merge_df)):\n",
    "    text = merge_df.loc[i, 'text']\n",
    "    summary = merge_df.loc[i, 'summary']\n",
    "    data = text + summary\n",
    "    dictionary.add_word(data)\n",
    "dictionary.sort_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29664"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryDataset(Dataset):\n",
    "    def __init__(self, data, dic, test = False):\n",
    "        self.data = data\n",
    "        self.dic = dic\n",
    "        self.test = test\n",
    "        self.tokenizer =  RegexpTokenizer(r'\\w+')\n",
    "        self.test = test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = '_sos_ ' + self.data.loc[idx, 'text'] + ' _eos_'\n",
    "        text_emb, text_word = self.get_emb(text)\n",
    "        if not self.test:\n",
    "            summary = self.data.loc[idx, 'summary'] + ' _eos_'\n",
    "            summary_emb, summary_word = self.get_emb(summary)\n",
    "            summary_word_id = self.get_summary_id_list(summary_word)\n",
    "            length = len(summary_word_id)\n",
    "            return  torch.tensor(text_emb), length, torch.tensor(summary_word_id)\n",
    "        id = self.data.loc[idx, 'id']\n",
    "        return torch.tensor(text_emb), text_word, id\n",
    "\n",
    "    def get_emb(self, data):\n",
    "        tokens = self.tokenizer.tokenize(data)\n",
    "        embeddings = []\n",
    "        words = []\n",
    "        for idx, token in enumerate(tokens):\n",
    "            token = token.lower()\n",
    "            emb = self.dic.get_emb(token)\n",
    "            if len(emb) > 0:\n",
    "                embeddings.append(emb)\n",
    "                words.append(token)\n",
    "        if len(embeddings) ==0:\n",
    "            return [[0.0 for i in range(300)]], [[]]\n",
    "        return embeddings, words\n",
    "    \n",
    "    def get_summary_id_list(self, words):\n",
    "        ans = []\n",
    "        for i in words:\n",
    "            word_id = self.dic.get_word_id(i)\n",
    "            if word_id != -1:\n",
    "                ans.append(word_id)\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    text_emb, length, summary_word_id = zip(*samples)\n",
    "    text_emb = pad_sequence(text_emb, batch_first=True)\n",
    "    summary_word_id = pad_sequence(summary_word_id, batch_first=True, padding_value=1)\n",
    "    return text_emb, length, summary_word_id\n",
    "\n",
    "def create_mini_batch_test(samples):\n",
    "    text_emb, text_word, id = zip(*samples)\n",
    "    text_emb = pad_sequence(text_emb, batch_first=True)\n",
    "    return text_emb, text_word, id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers =1, bidirectional=False, dropout = 0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                                     dropout= dropout, bidirectional=bidirectional, batch_first = True)\n",
    "        if bidirectional :\n",
    "            self.l1 = nn.Linear(2*hidden_size, hidden_size)\n",
    "        else:\n",
    "            self.l1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.init_weights()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.lstm.flatten_parameters()\n",
    "        out, (hn, cn) = self.lstm(x)# out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        if self.bidirectional:\n",
    "            hn = torch.cat((hn[0], hn[1]), 1)\n",
    "            hn = hn.unsqueeze(0)\n",
    "        hn = self.tanh(self.drop(self.l1(hn)))\n",
    "        return out, hn\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, p in self.lstm.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.orthogonal_(p)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(p, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[[1,2,3],[1,2,3]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= a.permute(1,0,2)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.repeat(1,2,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim, bidirectional = False):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(enc_hid_dim + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        #hidden = [batch size, 1, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src len, hidden_size]\n",
    "\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        \n",
    "        #repeat decoder hidden state src_len times\n",
    "\n",
    "        hidden = hidden.repeat(1, src_len, 1)  #hidden = [batch size, src len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), -1))) \n",
    "        \n",
    "        #energy = [batch size, src len, dec hid dim]\n",
    "\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        #attention= [batch size, src len]\n",
    "        return Func.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, word_size, num_layers =1, dropout = 0):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(input_size + hidden_size, word_size)\n",
    "        self.lstm = nn.LSTM(input_size + hidden_size, hidden_size, num_layers,\n",
    "                                     dropout= dropout, batch_first = True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "        self.attention = Attention(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, h = None, c= None):\n",
    "        self.lstm.flatten_parameters()\n",
    "        out, (hn, cn) = self.lstm(x, (h, c))# out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        return hn, cn\n",
    "\n",
    "    def predict(self, x, enc_outputs):\n",
    "        # x is hidden size\n",
    "        x = x.permute(1, 0, 2)\n",
    "        weight = self.attention(x, enc_outputs)\n",
    "        weight = weight.unsqueeze(1)\n",
    "        context = torch.bmm(weight, enc_outputs)\n",
    "        concat = torch.cat((context, x), -1) # torch.Size([B, 1, 600])\n",
    "        concat = concat.permute(1,0,2) # torch.Size([1, 16, 600])\n",
    "        out = self.l1(concat)\n",
    "        val, idx = out.max(-1)\n",
    "        return out, idx\n",
    "\n",
    "    def test(self, x):\n",
    "        out = self.l1(x)\n",
    "        return out\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, p in self.lstm.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.orthogonal_(p)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(p, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, word_size, num_layers=1, bidirectional=False, dropout = 0):\n",
    "        super(AutoEncoderRNN, self).__init__()\n",
    "        self.encoder = EncoderRNN(input_size, hidden_size, num_layers, bidirectional, dropout=dropout)\n",
    "        self.decoder = DecoderRNN(input_size, hidden_size, word_size, num_layers, dropout=dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 300\n",
    "hidden_size = 300\n",
    "batch_size = 16\n",
    "lr = 1e-3\n",
    "epoch = 3\n",
    "teacher_forcing = True\n",
    "num_layers = 1\n",
    "bidirectional = False\n",
    "dropout = 0\n",
    "device = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoderRNN(input_size, hidden_size, dictionary.n_words, bidirectional=bidirectional ).cuda(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SummaryDataset(train, dictionary)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, collate_fn = create_mini_batch ,drop_last = True, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = SummaryDataset(valid, embeddings_dict)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = batch_size, collate_fn = create_mini_batch ,drop_last = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])93544769287\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])511512756348\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])196544647217\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n",
      "torch.Size([1, 16, 300]) torch.Size([1, 16, 300]) torch.Size([16, 1, 600])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-4b308b8a643f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mtotal_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/peter/peter/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/peter/peter/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    iteration = 0\n",
    "    total_loss= 0\n",
    "    total_words = 0\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_f = nn.CrossEntropyLoss()\n",
    "    for text_emb, length, summary_word_id in train_loader:\n",
    "        text_emb = text_emb.float().cuda(device)\n",
    "        summary_word_id = summary_word_id.cuda(device)\n",
    "        batch_loss = 0\n",
    "        enc_outputs, context = model.encoder(text_emb)  #torch.Size([1, 5, 150])\n",
    "\n",
    "        hn = context\n",
    "        cn = Variable(torch.zeros(1, batch_size, hidden_size)).cuda(device)\n",
    "        \n",
    "        # first input with SOS token\n",
    "        SOS =  torch.tensor([[dictionary.get_emb('_sos_') for i in range(batch_size)]]).float().cuda(device)\n",
    "        inputs = torch.cat((context, SOS), 2) #torch.Size([1, B, 450])\n",
    "        inputs = inputs.permute(1,0,2)  # torch.Size([B, 1, 450])\n",
    "\n",
    "        words = SOS\n",
    "        index = 0\n",
    "        thres = int(summary_word_id.shape[1])\n",
    "\n",
    "        while True:\n",
    "            print(hn.shape, cn.shape, inputs.shape)\n",
    "            hn, cn = model.decoder(inputs, hn, cn)  # torch.Size([1, 5, 150])\n",
    "            values, predict = model.decoder.predict(hn, enc_outputs)  #torch.Size([1, 20, 98862]) torch.Size([1, 20])\n",
    "            for j in range(batch_size):\n",
    "                if length[j] >= index:\n",
    "                    labels = summary_word_id[:,index].long().cuda(device)\n",
    "                    loss = loss_f(values[0], labels)\n",
    "                    batch_loss += loss\n",
    "\n",
    "            # reconstruct input\n",
    "            if teacher_forcing :\n",
    "                words = [dictionary.id_to_word[labels.tolist()[j]] for j in range(batch_size)]\n",
    "                words = torch.tensor([dictionary.get_emb(words[j]) for j in range(len(words))]).float().cuda(device)\n",
    "            else:\n",
    "                words = [dictionary.id_to_word[predict.view(-1).tolist()[j]] for j in range(batch_size)]\n",
    "                words = torch.tensor([dictionary.get_emb(words[j]) for j in range(len(words))]).float().cuda(device)\n",
    "   \n",
    "            words = words.unsqueeze(0)\n",
    "            inputs = torch.cat((hn, words), -1).permute(1,0,2) # h[0] torch.Size([B, 98862])\n",
    "            if bidirectional:\n",
    "                hn = torch.cat((hn[:,:,:300], hn[:,:,300:]), 0)\n",
    "#             inputs = words.permute(1,0,2)\n",
    "            index += 1\n",
    "            \n",
    "            #if predict summary exceed thres\n",
    "            if index >= thres:\n",
    "                break\n",
    "\n",
    "        batch_loss.backward()\n",
    "        total_words += sum(length)\n",
    "        total_loss += batch_loss\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "#         opt_e.step()\n",
    "#         opt_e.zero_grad()\n",
    "#         opt_d.step()\n",
    "#         opt_d.zero_grad()\n",
    "        iteration += 1\n",
    "        print(f' Epoch : {i}, Iteration: {iteration}, batch_loss : {batch_loss/sum(length)}, avg_loss: {total_loss/ total_words}', end = '\\r')\n",
    "    valid_loss = validate()\n",
    "    if valid_loss < min_loss:\n",
    "        print(f'Validation loss improve from {min_loss} to {valid_loss} ')\n",
    "        min_loss = valid_loss\n",
    "        best_model = model\n",
    "        with open(f'./model/model_abtractive_0404_1500.pkl', 'wb') as output:\n",
    "            pickle.dump(best_model, output)\n",
    "    else:\n",
    "        print(f'Validation loss did not improve from original {min_loss} to {valid_loss} ')\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peter",
   "language": "python",
   "name": "peter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

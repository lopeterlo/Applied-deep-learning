{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Func\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "with open('./data/train.jsonl', 'r') as f:\n",
    "    file = list(f)\n",
    "for row in file:\n",
    "    train.append(json.loads(row))\n",
    "    \n",
    "valid = []\n",
    "with open('./data/valid.jsonl', 'r') as f:\n",
    "    file = list(f)\n",
    "for row in file:\n",
    "    valid.append(json.loads(row))    \n",
    "    \n",
    "test = []\n",
    "with open('./data/test.jsonl', 'r') as f:\n",
    "    file = list(f)\n",
    "for row in file:\n",
    "    test.append(json.loads(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary = {}\n",
    "# dictionary_rev = {}\n",
    "# for row in train:\n",
    "#     tokens = nlp(row['text'])\n",
    "#     tokens = [token for token in tokens if token.is_alpha]\n",
    "#     for tk in tokens:\n",
    "#         if not dictionary.get(tk):\n",
    "#             dictionary[tk] = len(dictionary)\n",
    "#             dictionary_rev[len(dictionary)] = tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71604"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "with open(\"./glove.6B/glove.6B.300d.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, w2v, test = False):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.w2v = w2v\n",
    "        self.test = test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if not self.test:\n",
    "            embeddings, labels = self.tokenize(self.data[idx])\n",
    "            return embeddings, labels\n",
    "            \n",
    "        else:\n",
    "            embeddings, id, words_pos, sentence = self.tokenize_test(self.data[idx])\n",
    "            return embeddings, id, words_pos, sentence\n",
    "    \n",
    "    def tokenize(self, inputs):\n",
    "        data = inputs['text']\n",
    "        sent_b = inputs['sent_bounds']\n",
    "        ans_idx = inputs['extractive_summary']\n",
    "\n",
    "        all_embeddings = []\n",
    "        labels = []\n",
    "        if len(inputs['sent_bounds'])  > 1:\n",
    "            n_sam_idx = random.randint(0, len(inputs['sent_bounds'])-1)\n",
    "            while n_sam_idx == ans_idx:\n",
    "                n_sam_idx = random.randint(0, len(inputs['sent_bounds'])-1)\n",
    "            \n",
    "            if n_sam_idx > ans_idx:\n",
    "                sent_b = [sent_b[ans_idx], sent_b[n_sam_idx]]\n",
    "                ans_idx = 0\n",
    "            else:\n",
    "                sent_b = [sent_b[n_sam_idx], sent_b[ans_idx]]\n",
    "                ans_idx = 1\n",
    "        for idx, pos in enumerate(sent_b):\n",
    "            tokens = self.tokenizer(data[pos[0]: pos[1]])\n",
    "            embeddings, words = self.clean(tokens)\n",
    "            all_embeddings += embeddings\n",
    "            if idx == ans_idx:\n",
    "                labels += [1 for i in range(len(words))]\n",
    "            else:\n",
    "                labels += [0 for i in range(len(words))]\n",
    "\n",
    "        return torch.tensor(all_embeddings), torch.tensor(labels)\n",
    "    \n",
    "    def tokenize_test(self, inputs):\n",
    "        data = inputs['text']\n",
    "        sent_b = inputs['sent_bounds']\n",
    "        all_embeddings = []\n",
    "        id = inputs['id']\n",
    "        word_pos = []\n",
    "        all_sentences = []\n",
    "        for idx, pos in enumerate(sent_b):\n",
    "            all_sentences.append(data[pos[0]: pos[1]][:-2])\n",
    "            tokens = self.tokenizer(data[pos[0]: pos[1]])\n",
    "            embeddings, words = self.clean(tokens)\n",
    "            all_embeddings += embeddings\n",
    "            word_pos += [idx for i in range(len(words))]\n",
    "        return torch.tensor(all_embeddings), id, word_pos, all_sentences\n",
    "    \n",
    "    \n",
    "    def clean(self, tokens):\n",
    "        embeddings = []\n",
    "        words = []\n",
    "        for idx, token in enumerate(tokens):\n",
    "            if token.is_alpha and token.text.lower() in self.w2v.keys():\n",
    "                token = token.lower_\n",
    "                embeddings.append(self.w2v[token])\n",
    "                words.append(token)\n",
    "        return embeddings, words\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    # 測試集有 labels\n",
    "    tokens_tensors,  labels = zip(*samples)\n",
    "    # zero pad 到同一序列長度\n",
    "    try:\n",
    "        tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)\n",
    "        labels = pad_sequence(labels, batch_first=True)\n",
    "#         print(len(tokens_tensors), len(labels))\n",
    "    except :\n",
    "        return [],[]\n",
    "    return tokens_tensors, labels\n",
    "\n",
    "def create_mini_batch_test(samples):\n",
    "    tokens_tensors,  words, words_pos, sentence = zip(*samples)\n",
    "    try:\n",
    "        tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)\n",
    "    except :\n",
    "        return [], [], [], []\n",
    "    return tokens_tensors, words, words_pos, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_model(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size = 200, n_layers = 1, drop_prob=0.2, bidirectional = False):\n",
    "        super(LSTM_model, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.model = nn.LSTM(input_size, hidden_size, n_layers,  batch_first = True, bidirectional= bidirectional)\n",
    "        self.relu = nn.ReLU()\n",
    "        if bidirectional:\n",
    "            self.l1 = nn.Linear(hidden_size * 2, 128)\n",
    "            self.l2 = nn.Linear(128, 32)\n",
    "            self.l3 = nn.Linear(32, 2)\n",
    "        else:\n",
    "            self.l1 = nn.Linear(hidden_size, 128)\n",
    "            self.l2 = nn.Linear(128, 32)\n",
    "            self.l3 = nn.Linear(32, 2)\n",
    "        self.init_hidden()\n",
    "        \n",
    "    def forward(self, x, h= None):\n",
    "        x, (hn, cn) = self.model(x, h)\n",
    "        l1 = self.l1(x)\n",
    "        l2 = self.l2(l1)\n",
    "        l3 = self.l3(l2)\n",
    "        l3 = l3.view(-1, l3.size(2))\n",
    "#         out = Func.softmax(l3, dim=2) # along rows\n",
    "        return l3\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        for name, p in self.model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.orthogonal_(p)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(p, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 300\n",
    "batch_size = 64\n",
    "nlp = en_core_web_sm.load()\n",
    "train_dataset = SummaryDataset(train, nlp, embeddings_dict)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, collate_fn=create_mini_batch, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM_model(input_size)\n",
    "model = model.cuda()\n",
    "index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x, y in train_loader:\n",
    "    if len(x) == 0:\n",
    "        continue\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    pred = model(x)\n",
    "    temp_loss = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "#     print(x.shape, y.shape)     torch.Size([2, 56, 300]) torch.Size([2, 56])\n",
    "#     print(pred.shape)           torch.Size([112, 2])\n",
    "    w_1 = sum([sum(y[i]) for i in range(len(y))]).tolist()\n",
    "    w_0 = sum([len(y[i]) for i in range(len(y))])\n",
    "#     total = w_0 + w_1\n",
    "#     w_0 = w_0 / total\n",
    "#     w_1 = w_1 / total\n",
    "#     weight = torch.tensor((w_1, w_0))\n",
    "#     loss_f = nn.CrossEntropyLoss(weight = weight.float())\n",
    "#         loss_f = nn.CrossEntropyLoss()\n",
    "    loss_f = nn.BCEWithLogitsLoss(pos_weight = torch.tensor(w_0, w_1))\n",
    "    y = y.view(-1)\n",
    "    true = torch.tensor([[0,1] if j == 1 else[1,0] for j in y]).cuda().float()\n",
    "\n",
    "    loss = loss_f(pred, true)\n",
    "    loss.backward(retain_graph=True)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    total += len(y)\n",
    "    total_loss += loss.item() * len(y)\n",
    "\n",
    "    print(f'Iteration : {index+1} , Loss : {total_loss/total } ', end = '\\r')\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 300\n",
    "batch_size = 1\n",
    "test_dataset = SummaryDataset(test, nlp, embeddings_dict, test = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, collate_fn=create_mini_batch_test)\n",
    "\n",
    "\n",
    "valid_dataset = SummaryDataset(valid, nlp, embeddings_dict, test = True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, collate_fn=create_mini_batch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019999\r"
     ]
    }
   ],
   "source": [
    "prediction = ''\n",
    "for x, id, words_pos, sentence in valid_loader:\n",
    "    if len(x[0]) ==0:\n",
    "        prediction += json.dumps({\"id\":id[0], \"predict_sentence_index\": [0]}) + '\\n'\n",
    "        continue\n",
    "#     print(x.shape) # batch_size, sentence_length, word_enb torch.Size([16, 282, 300])\n",
    "    x = x.cuda()\n",
    "    pred = model(x) # torch.Size([16, 282, 2])\n",
    "    out = Func.softmax(pred, dim=1) # torch.Size([282, 2])\n",
    "    values, indexs = out.max(-1)\n",
    "    ans = indexs.tolist()\n",
    "    zipped = zip(words_pos[0], ans)\n",
    "    cum_num = defaultdict(lambda:0,{})\n",
    "    for (i,j) in zipped:\n",
    "        if j == 1:\n",
    "            cum_num[i] += 1\n",
    "    cum_num = sorted(cum_num.items(), key= lambda x: x[1], reverse = True)\n",
    "    extractive_pred = [i for i, j in cum_num]\n",
    "    prediction += json.dumps({\"id\":id[0], \"predict_sentence_index\": extractive_pred[:2]}) + '\\n'\n",
    "    \n",
    "    print(id[0], end ='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prediction.json','w') as f:\n",
    "    f.write(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

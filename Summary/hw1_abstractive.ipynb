{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Func\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('./data/train.jsonl', lines= True)\n",
    "valid = pd.read_json('./data/valid.jsonl', lines= True)\n",
    "test = pd.read_json('./data/test.jsonl', lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>summary</th>\n",
       "      <th>text</th>\n",
       "      <th>sent_bounds</th>\n",
       "      <th>extractive_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1000000</td>\n",
       "      <td>A seven-hundred-year old oak gate at Salisbury...</td>\n",
       "      <td>The Grade I listed Harnham Gate was hit by a w...</td>\n",
       "      <td>[[0, 107], [107, 255], [255, 362]]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                            summary  \\\n",
       "0  1000000  A seven-hundred-year old oak gate at Salisbury...   \n",
       "\n",
       "                                                text  \\\n",
       "0  The Grade I listed Harnham Gate was hit by a w...   \n",
       "\n",
       "                          sent_bounds  extractive_summary  \n",
       "0  [[0, 107], [107, 255], [255, 362]]                   1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "with open(\"./glove.6B/glove.6B.300d.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add SOS and EOS\n",
    "embeddings_dict['_sos_'] =  np.random.rand(300, )\n",
    "embeddings_dict['_eos_'] =  np.random.rand(300, )\n",
    "embeddings_dict['_unk_'] =  np.random.rand(300, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class words_dict():\n",
    "    def __init__(self, glove):\n",
    "        self.word_count = {}\n",
    "        self.id_to_word = {0: '_sos_', 1: '_eos_', 2: '_unk_'}\n",
    "        self.word_to_id = {'_sos_': 0, '_eos_': 1, '_unk_': 2}\n",
    "        self.n_words = 3\n",
    "        self.tokenizer =  RegexpTokenizer(r'\\w+')\n",
    "        self.remain_id = []\n",
    "        self.glove = glove\n",
    "        \n",
    "    def add_word(self, sentence):\n",
    "        tokens = self.tokenizer.tokenize(sentence)\n",
    "        for token in tokens:\n",
    "            token = token.lower()\n",
    "            if token in self.glove.keys():\n",
    "                if not self.word_to_id.get(token) :\n",
    "                    self.word_to_id[token] = self.n_words\n",
    "                    self.id_to_word[self.n_words] = token\n",
    "                    self.n_words += 1\n",
    "                    self.word_count[token] = 1\n",
    "                else:\n",
    "                    self.word_count[token] += 1\n",
    "                    \n",
    "    def sort_dict(self):\n",
    "        self.remain_id.append(0)\n",
    "        self.remain_id.append(1)\n",
    "        self.remain_id.append(2)\n",
    "        sort_d = sorted(self.word_count.items(), key = lambda x: x[1], reverse = True)[:int(self.n_words *0.3)]\n",
    "        for (i, j) in sort_d:\n",
    "            self.remain_id.append(self.word_to_id[i])\n",
    "\n",
    "        self.reconstruct()\n",
    "    \n",
    "    \n",
    "    def reconstruct(self):\n",
    "                # reconstruct dict\n",
    "        n_words =  3\n",
    "        id_to_word = {0: '_sos_', 1: '_eos_', 2: '_unk_'}\n",
    "        word_to_id = {'_sos_': 0, '_eos_': 1, '_unk_': 2}\n",
    "        for i in self.remain_id:\n",
    "            if not word_to_id.get(i):\n",
    "                word_to_id[self.id_to_word[i]] = n_words\n",
    "                id_to_word[n_words] = self.id_to_word[i]\n",
    "                n_words += 1\n",
    "        self.n_words = n_words\n",
    "        self.id_to_word = id_to_word\n",
    "        self.word_to_id = word_to_id\n",
    "        self.remain_id = [i for i in range(n_words)]\n",
    "        \n",
    "    def get_emb(self, data):\n",
    "        if self.word_to_id.get(data, -1) != -1:\n",
    "            if self.word_to_id[data] in self.remain_id:\n",
    "                return self.glove[data]\n",
    "        return self.glove['_unk_']\n",
    "        \n",
    "    def get_word_id(self, data):\n",
    "        if data == []:\n",
    "            return -1\n",
    "        if self.word_to_id.get(data, -1) != -1:\n",
    "            if self.word_to_id[data] in self.remain_id:\n",
    "                return self.word_to_id[data]\n",
    "        return 2\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91604, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_df = train.append(valid, ignore_index= True)\n",
    "merge_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = words_dict(embeddings_dict)\n",
    "for i in range(len(merge_df)):\n",
    "    text = merge_df.loc[i, 'text']\n",
    "    summary = merge_df.loc[i, 'summary']\n",
    "    data = text + summary\n",
    "    dictionary.add_word(data)\n",
    "dictionary.sort_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29664"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryDataset(Dataset):\n",
    "    def __init__(self, data, dic, test = False):\n",
    "        self.data = data\n",
    "        self.dic = dic\n",
    "        self.test = test\n",
    "        self.tokenizer =  RegexpTokenizer(r'\\w+')\n",
    "        self.test = test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = '_sos_ ' + self.data.loc[idx, 'text'] + ' _eos_'\n",
    "        text_emb, text_word = self.get_emb(text)\n",
    "        if not self.test:\n",
    "            summary = self.data.loc[idx, 'summary'] + ' _eos_'\n",
    "            summary_emb, summary_word = self.get_emb(summary)\n",
    "            summary_word_id = self.get_summary_id_list(summary_word)\n",
    "            length = len(summary_word_id)\n",
    "            return  torch.tensor(text_emb), length, torch.tensor(summary_word_id)\n",
    "        id = self.data.loc[idx, 'id']\n",
    "        return torch.tensor(text_emb), text_word, id\n",
    "\n",
    "    def get_emb(self, data):\n",
    "        tokens = self.tokenizer.tokenize(data)\n",
    "        embeddings = []\n",
    "        words = []\n",
    "        for idx, token in enumerate(tokens):\n",
    "            token = token.lower()\n",
    "            emb = self.dic.get_emb(token)\n",
    "            if len(emb) > 0:\n",
    "                embeddings.append(emb)\n",
    "                words.append(token)\n",
    "        if len(embeddings) ==0:\n",
    "            return [[0.0 for i in range(300)]], [[]]\n",
    "        return embeddings, words\n",
    "    \n",
    "    def get_summary_id_list(self, words):\n",
    "        ans = []\n",
    "        for i in words:\n",
    "            word_id = self.dic.get_word_id(i)\n",
    "            if word_id != -1:\n",
    "                ans.append(word_id)\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    text_emb, length, summary_word_id = zip(*samples)\n",
    "    text_emb = pad_sequence(text_emb, batch_first=True)\n",
    "    summary_word_id = pad_sequence(summary_word_id, batch_first=True, padding_value=1)\n",
    "    return text_emb, length, summary_word_id\n",
    "\n",
    "def create_mini_batch_test(samples):\n",
    "    text_emb, text_word, id = zip(*samples)\n",
    "    text_emb = pad_sequence(text_emb, batch_first=True)\n",
    "    return text_emb, text_word, id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers =1, bidirectional=False, dropout = 0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                            dropout= dropout, bidirectional=bidirectional, batch_first = True)\n",
    "        if bidirectional :\n",
    "            self.l1 = nn.Linear(2*hidden_size, hidden_size)\n",
    "        else:\n",
    "            self.l1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tan = nn.Tanh()\n",
    "        self.init_weights()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.lstm.flatten_parameters()\n",
    "        out, (hn, cn) = self.lstm(x)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        if  self.bidirectional:\n",
    "            hn = torch.cat((hn[0], hn[1]),1)\n",
    "            hn = hn.unsqueeze(0)\n",
    "        hn = self.tan(self.drop(self.l1(hn)))\n",
    "        return hn\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, p in self.lstm.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.orthogonal_(p)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(p, 0)\n",
    "                \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers =1, dropout = 0):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Linear(input_size + hidden_size, dictionary.n_words)\n",
    "        self.lstm = nn.LSTM(input_size + hidden_size, hidden_size, num_layers,\n",
    "                            dropout= dropout, batch_first = True)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x, h = None, c= None):\n",
    "        self.lstm.flatten_parameters()\n",
    "        out, (hn, cn) = self.lstm(x, (h, c))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        return hn, cn\n",
    "    \n",
    "    def predict(self, x):\n",
    "        out = self.l1(x)\n",
    "        val, idx = out.max(-1)\n",
    "        return out, idx\n",
    "    \n",
    "    def test(self, x):\n",
    "        out = self.l1(x)\n",
    "        return out\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for name, p in self.lstm.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.orthogonal_(p)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(p, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, bidirectional=False, dropout = 0):\n",
    "        super(AutoEncoderRNN, self).__init__()\n",
    "        self.encoder = EncoderRNN(input_size, hidden_size, num_layers, bidirectional, dropout=dropout)\n",
    "        self.decoder = DecoderRNN(input_size, hidden_size, num_layers, dropout=dropout)  \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 300\n",
    "hidden_size = 300\n",
    "batch_size = 16\n",
    "lr = 1e-3\n",
    "epoch = 3\n",
    "teacher_forcing = True\n",
    "num_layers = 1\n",
    "bidirectional = True\n",
    "dropout = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoderRNN(input_size, hidden_size, bidirectional=bidirectional).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_dataset = SummaryDataset(train, dictionary)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, collate_fn = create_mini_batch ,drop_last = True, shuffle= True)\n",
    "\n",
    "valid_dataset = SummaryDataset(valid, dictionary)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = batch_size, collate_fn = create_mini_batch, drop_last = True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 600])\n",
      "torch.Size([16, 1, 600]), batch_loss : 8.522150039672852, avg_loss: 8.522150039672852\n",
      "torch.Size([16, 1, 600]), batch_loss : 8.033958435058594, avg_loss: 8.264249801635742\n",
      "torch.Size([16, 1, 600]), batch_loss : 7.623191833496094, avg_loss: 8.064986228942871\n",
      "torch.Size([16, 1, 600]), batch_loss : 7.6953582763671875, avg_loss: 7.972710609436035\n",
      "torch.Size([16, 1, 600]), batch_loss : 6.99948263168335, avg_loss: 7.7895612716674805\n",
      "torch.Size([16, 1, 600]), batch_loss : 6.935104846954346, avg_loss: 7.649357318878174\n",
      "torch.Size([16, 1, 600]), batch_loss : 7.29809045791626, avg_loss: 7.600976943969727\n",
      "torch.Size([16, 1, 600]), batch_loss : 6.892031669616699, avg_loss: 7.5090436935424805\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-89344fca5f52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummary_word_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0mbatch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "min_loss = 10000000000\n",
    "for i in range(epoch):\n",
    "    iteration = 0\n",
    "    total_loss= 0\n",
    "    total_words = 0\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#     opt_e = torch.optim.Adam(model.encoder.parameters(), lr=lr)\n",
    "#     opt_d = torch.optim.Adam(model.decoder.parameters(), lr=lr)\n",
    "    loss_f = nn.CrossEntropyLoss()\n",
    "    for text_emb, length, summary_word_id in train_loader:\n",
    "        text_emb = text_emb.float().cuda()\n",
    "        summary_word_id = summary_word_id.cuda()\n",
    "        batch_loss = 0\n",
    "        context = model.encoder(text_emb)  #torch.Size([1, 5, 150])\n",
    "        hn = context\n",
    "#       hn = Variable(torch.zeros(1, batch_size, hidden_size)).cuda()\n",
    "        cn = Variable(torch.zeros(1, batch_size, hidden_size)).cuda()\n",
    "        \n",
    "        # first input with SOS token\n",
    "        SOS =  torch.tensor([[dictionary.get_emb('_sos_') for i in range(batch_size)]]).float().cuda()\n",
    "        inputs = torch.cat((context, SOS), 2) #torch.Size([1, B, 450])\n",
    "        inputs = inputs.permute(1,0,2)  # torch.Size([B, 1, 450])\n",
    "#         inputs = SOS.permute(1,0,2)\n",
    "        words = SOS\n",
    "        index = 0\n",
    "        thres = int(summary_word_id.shape[1])\n",
    "\n",
    "        while True:\n",
    "            hn, cn = model.decoder(inputs, hn, cn)  # torch.Size([1, 5, 150])\n",
    "            combined = torch.cat((context, hn), -1)\n",
    "            values, predict = model.decoder.predict(combined)  #torch.Size([1, 20, 98862]) torch.Size([1, 20])\n",
    "\n",
    "           \n",
    "            for j in range(batch_size):\n",
    "                if length[j] >= index:\n",
    "                    labels = summary_word_id[:,index].long().cuda()\n",
    "                    loss = loss_f(values[0], labels)\n",
    "                    batch_loss += loss\n",
    "\n",
    "\n",
    "            # reconstruct input\n",
    "            if teacher_forcing :\n",
    "                words = [dictionary.id_to_word[labels.tolist()[j]] for j in range(batch_size)]\n",
    "                words = torch.tensor([dictionary.get_emb(words[j]) for j in range(len(words))]).float().cuda()\n",
    "            else:\n",
    "                words = [dictionary.id_to_word[predict.view(-1).tolist()[j]] for j in range(batch_size)]\n",
    "                words = torch.tensor([dictionary.get_emb(words[j]) for j in range(len(words))]).float().cuda()\n",
    "   \n",
    "            words = words.unsqueeze(0)\n",
    "            inputs = torch.cat((hn, words), -1).permute(1,0,2) # h[0] torch.Size([B, 98862])\n",
    "            index += 1\n",
    "            \n",
    "            #if predict summary exceed thres\n",
    "            if index >= thres:\n",
    "                break\n",
    "\n",
    "        batch_loss.backward()\n",
    "        total_words += sum(length)\n",
    "        total_loss += batch_loss\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "#         opt_e.step()\n",
    "#         opt_e.zero_grad()\n",
    "#         opt_d.step()\n",
    "#         opt_d.zero_grad()\n",
    "        iteration += 1\n",
    "        print(f' Epoch : {i}, Iteration: {iteration}, batch_loss : {batch_loss/sum(length)}, avg_loss: {total_loss/ total_words}', end = '\\r')\n",
    "    valid_loss = validate()\n",
    "    if valid_loss < min_loss:\n",
    "        print(f'Validation loss improve from {min_loss} to {valid_loss} ')\n",
    "        min_loss = valid_loss\n",
    "        best_model = model\n",
    "        with open(f'./model/model_abtractive_0404_1500.pkl', 'wb') as output:\n",
    "            pickle.dump(best_model, output)\n",
    "    else:\n",
    "        print(f'Validation loss did not improve from original {min_loss} to {valid_loss} ')\n",
    "        break\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'./model/model_abtractive_0404_1500.pkl', 'wb') as output:\n",
    "#     pickle.dump(best_model, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():\n",
    "    with torch.no_grad():\n",
    "        iteration = 0\n",
    "        total_loss= 0\n",
    "        total_words = 0\n",
    "        loss_f = nn.CrossEntropyLoss()\n",
    "        for text_emb, length, summary_word_id in valid_loader:\n",
    "            text_emb = text_emb.float().cuda()\n",
    "            summary_word_id = summary_word_id.cuda()\n",
    "            batch_loss = 0\n",
    "            context = model.encoder(text_emb)  #torch.Size([1, 5, 150])\n",
    "            if bidirectional:\n",
    "        #             hn = torch.cat((context[:,:,:300], context[:,:,300:]), 0)\n",
    "                hn = Variable(torch.zeros(2, batch_size, hidden_size)).cuda()\n",
    "                cn = Variable(torch.zeros(2, batch_size, hidden_size)).cuda()\n",
    "            else:\n",
    "                hn = context\n",
    "#                 hn = Variable(torch.zeros(1, batch_size, hidden_size)).cuda()\n",
    "                cn = Variable(torch.zeros(1, batch_size, hidden_size)).cuda()\n",
    "\n",
    "            # first input with SOS token\n",
    "            SOS =  torch.tensor([[dictionary.get_emb('_sos_') for i in range(batch_size)]]).float().cuda()\n",
    "            inputs = torch.cat((context, SOS), 2) #torch.Size([1, B, 450])\n",
    "            inputs = inputs.permute(1,0,2)  # torch.Size([B, 1, 450])\n",
    "        #         inputs = SOS.permute(1,0,2)\n",
    "\n",
    "            words = SOS\n",
    "            index = 0\n",
    "            thres = int(summary_word_id.shape[1])\n",
    "\n",
    "            while True:\n",
    "                hn, cn = model.decoder(inputs, hn, cn)  # torch.Size([1, 5, 150])\n",
    "                combined = torch.cat((context, hn), -1)\n",
    "                values, predict = model.decoder.predict(combined)  #torch.Size([1, 20, 98862]) torch.Size([1, 20])\n",
    "\n",
    "\n",
    "                for j in range(batch_size):\n",
    "                    if length[j] >= index:\n",
    "                        labels = summary_word_id[:,index].long().cuda()\n",
    "                        loss = loss_f(values[0], labels)\n",
    "                        batch_loss += loss\n",
    "\n",
    "                # reconstruct input\n",
    "                if teacher_forcing :\n",
    "                    words = [dictionary.id_to_word[labels.tolist()[j]] for j in range(batch_size)]\n",
    "                    words = torch.tensor([dictionary.get_emb(words[j]) for j in range(len(words))]).float().cuda()\n",
    "                else:\n",
    "                    words = [dictionary.id_to_word[predict.view(-1).tolist()[j]] for j in range(batch_size)]\n",
    "                    words = torch.tensor([dictionary.get_emb(words[j]) for j in range(len(words))]).float().cuda()\n",
    "\n",
    "                words = words.unsqueeze(0)\n",
    "                inputs = torch.cat((hn, words), -1).permute(1,0,2) # h[0] torch.Size([B, 98862])\n",
    "                if bidirectional:\n",
    "                    hn = torch.cat((hn[:,:,:300], hn[:,:,300:]), 0)\n",
    "        #             inputs = words.permute(1,0,2)\n",
    "                index += 1\n",
    "                #if predict summary exceed thres\n",
    "                if index >= thres:\n",
    "                    break\n",
    "            total_words += sum(length)\n",
    "            total_loss += batch_loss\n",
    "            iteration += 1\n",
    "            print(f' Validation Iteration: {iteration}, batch_loss : {batch_loss/sum(length)}, avg_loss: {total_loss/ total_words}', end = '\\r')\n",
    "        return total_loss/total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./model/model_abtractive_0404_1500.pkl', 'rb') as inputs:\n",
    "    model = pickle.load(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "test_dataset = SummaryDataset(valid, dictionary, test = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, collate_fn = create_mini_batch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loop, Iteration: 399\r"
     ]
    }
   ],
   "source": [
    "prediction = ''\n",
    "iteration = 0\n",
    "with torch.no_grad():\n",
    "    for text_emb, text_word, id in test_loader:\n",
    "        text_emb = text_emb.float().cuda()\n",
    "        context = model.encoder(text_emb)  #torch.Size([1, 5, 150])\n",
    "        \n",
    "        hn = context\n",
    "#         hn = Variable(torch.zeros(1, batch_size, hidden_size)).cuda()\n",
    "        cn = Variable(torch.zeros(1, batch_size, hidden_size)).cuda()\n",
    "        \n",
    "        SOS =  torch.tensor([[dictionary.get_emb('_sos_') for i in range(batch_size)]]).float().cuda()\n",
    "        inputs = torch.cat((context, SOS), 2) #torch.Size([1, 5, 450])\n",
    "        inputs = inputs.permute(1,0,2)\n",
    "#         inputs = SOS.permute(1,0,2)  # torch.Size([5, 1, 450])\n",
    "        words = SOS\n",
    "\n",
    "#       stop criteria\n",
    "        thres = len(text_emb[0]) * 0.3\n",
    "        ans = [[] for i in range(batch_size)]\n",
    "        index = 0\n",
    "        while True:\n",
    "            hn, cn = model.decoder(inputs, hn, cn)  # torch.Size([1, 5, 150])\n",
    "            combined = torch.cat((context, hn), -1)\n",
    "            values, predict = model.decoder.predict(combined)  #torch.Size([1, 20, 98862]) torch.Size([1, 20])\n",
    "        \n",
    "            val, pred = values.topk(3)\n",
    "#             print(pred)\n",
    "            for i in range(batch_size):\n",
    "                pred_word = dictionary.id_to_word[predict.tolist()[0][i]]\n",
    "                ans[i].append(pred_word)\n",
    "            \n",
    "            words = torch.tensor([dictionary.get_emb(ans[j][index]) for j in range(batch_size)]).float().cuda()\n",
    "            words = words.unsqueeze(0)\n",
    "            \n",
    "            inputs = torch.cat((hn, words), -1).permute(1,0,2)\n",
    "#             inputs = words.permute(1,0,2)\n",
    "            index += 1\n",
    "            \n",
    "            #if predict summary exceed 40 words then stop\n",
    "            if index >= thres:\n",
    "                break\n",
    "\n",
    "        print(f'Validation loop, Iteration: {iteration}', end = '\\r')\n",
    "        iteration += 1\n",
    "        for idx in range(batch_size):\n",
    "            try:\n",
    "                eos_idx = ans[idx].index('_eos_') + 1\n",
    "                ans[idx] =  ans[idx][:eos_idx]\n",
    "                prediction += json.dumps({\"id\":str(id[idx]), \"predict\": ' '.join(ans[idx])}) + '\\n'\n",
    "            except:\n",
    "                prediction += json.dumps({\"id\":str(id[idx]), \"predict\": ' '.join(ans[idx])}) + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prediction_ab_0404.json','w') as f:\n",
    "    f.write(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neet to improve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. padding loss recalculation (done)\n",
    "2. torch.save\n",
    "3. pytorch lightning\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peter",
   "language": "python",
   "name": "peter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
